{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPmF1PIOWTL5FCmRTapuG65",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DanielWarfield1/common_core_example_explorer/blob/main/2024_10_03_HardOutput.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Okie Dokie\n",
        "here's the sitch wade\n",
        "\n",
        "I made https://github.com/DanielWarfield1/common_core_example_explorer which currently only supports hard coded queries and responses via a mock API, so I'm re-structuring this to create properly structured data so that I can put it in that mock API."
      ],
      "metadata": {
        "id": "Gkb3Vds31HM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install groundx-python-sdk --upgrade\n",
        "!pip install langchain-community langchain-core langchain_openai\n",
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLyMPS200pjX",
        "outputId": "48e349db-995a-4c01-f5c4-66fdd8a8f32f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting groundx-python-sdk\n",
            "  Downloading groundx_python_sdk-1.3.26-py3-none-any.whl.metadata (33 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.4 in /usr/local/lib/python3.10/dist-packages (from groundx-python-sdk) (3.10.8)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from groundx-python-sdk) (2024.8.30)\n",
            "Collecting cryptography<43.0.0,>=42.0.5 (from groundx-python-sdk)\n",
            "  Downloading cryptography-42.0.8-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: frozendict<3.0.0,>=2.3.4 in /usr/local/lib/python3.10/dist-packages (from groundx-python-sdk) (2.4.4)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from groundx-python-sdk) (2.8.2)\n",
            "Requirement already satisfied: typing_extensions<5.0.0,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from groundx-python-sdk) (4.12.2)\n",
            "Requirement already satisfied: urllib3<3.0.0,>=1.26.18 in /usr/local/lib/python3.10/dist-packages (from groundx-python-sdk) (2.2.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.4->groundx-python-sdk) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.4->groundx-python-sdk) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.4->groundx-python-sdk) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.4->groundx-python-sdk) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.4->groundx-python-sdk) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.4->groundx-python-sdk) (1.13.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.4->groundx-python-sdk) (4.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography<43.0.0,>=42.0.5->groundx-python-sdk) (1.17.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.8.2->groundx-python-sdk) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography<43.0.0,>=42.0.5->groundx-python-sdk) (2.22)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.4->groundx-python-sdk) (3.10)\n",
            "Downloading groundx_python_sdk-1.3.26-py3-none-any.whl (358 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m358.5/358.5 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cryptography-42.0.8-cp39-abi3-manylinux_2_28_x86_64.whl (3.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: cryptography, groundx-python-sdk\n",
            "  Attempting uninstall: cryptography\n",
            "    Found existing installation: cryptography 43.0.1\n",
            "    Uninstalling cryptography-43.0.1:\n",
            "      Successfully uninstalled cryptography-43.0.1\n",
            "Successfully installed cryptography-42.0.8 groundx-python-sdk-1.3.26\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting langchain-core\n",
            "  Downloading langchain_core-0.3.8-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.2.1-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.35)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.10.8)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting langchain<0.4.0,>=0.3.1 (from langchain-community)\n",
            "  Downloading langchain-0.3.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.125 (from langchain-community)\n",
            "  Downloading langsmith-0.1.131-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.26.4)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.5.2-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.32.3)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain-community)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (24.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (2.9.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (4.12.2)\n",
            "Collecting openai<2.0.0,>=1.40.0 (from langchain_openai)\n",
            "  Downloading openai-1.51.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain_openai)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.13.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain<0.4.0,>=0.3.1->langchain-community)\n",
            "  Downloading langchain_text_splitters-0.3.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting httpx<1,>=0.23.0 (from langsmith<0.2.0,>=0.1.125->langchain-community)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.125->langchain-community)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.2.0,>=0.1.125->langchain-community)\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (1.7.0)\n",
            "Collecting jiter<1,>=0.4.0 (from openai<2.0.0,>=1.40.0->langchain_openai)\n",
            "  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain_openai) (4.66.5)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core) (2.23.4)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.9.11)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->langchain_openai) (1.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community)\n",
            "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langchain_community-0.3.1-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.8-py3-none-any.whl (400 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-0.2.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading langchain-0.3.2-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langsmith-0.1.131-py3-none-any.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.51.0-py3-none-any.whl (383 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.5/383.5 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.5.2-py3-none-any.whl (26 kB)\n",
            "Downloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading langchain_text_splitters-0.3.0-py3-none-any.whl (25 kB)\n",
            "Downloading marshmallow-3.22.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tenacity, python-dotenv, orjson, mypy-extensions, marshmallow, jsonpointer, jiter, h11, typing-inspect, tiktoken, requests-toolbelt, jsonpatch, httpcore, pydantic-settings, httpx, dataclasses-json, openai, langsmith, langchain-core, langchain-text-splitters, langchain_openai, langchain, langchain-community\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "Successfully installed dataclasses-json-0.6.7 h11-0.14.0 httpcore-1.0.6 httpx-0.27.2 jiter-0.5.0 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.2 langchain-community-0.3.1 langchain-core-0.3.8 langchain-text-splitters-0.3.0 langchain_openai-0.2.1 langsmith-0.1.131 marshmallow-3.22.0 mypy-extensions-1.0.0 openai-1.51.0 orjson-3.10.7 pydantic-settings-2.5.2 python-dotenv-1.0.1 requests-toolbelt-1.0.0 tenacity-8.5.0 tiktoken-0.8.0 typing-inspect-0.9.0\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.51.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.5.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npjTdce10fg6"
      },
      "outputs": [],
      "source": [
        "bucket_id = 11644"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "from groundx import Groundx\n",
        "from google.colab import userdata\n",
        "\n",
        "groundx = Groundx(\n",
        "    api_key=userdata.get('GroundXAPIKey_daniel.warfield')\n",
        ")"
      ],
      "metadata": {
        "id": "wO0S6l3p0q3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "from groundx import Groundx\n",
        "\n",
        "print(f'{bucket_id} {type(bucket_id)}')\n",
        "\n",
        "response = groundx.search.content(\n",
        "    id=bucket_id,\n",
        "    query=\"I need examples of calculating area\"\n",
        ")"
      ],
      "metadata": {
        "id": "oVH_uRoE0ul7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6349933-b513-4b54-de0a-3835ae16341c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11644 <class 'int'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "relevent_docs = set([])\n",
        "for res in response.body['search']['results']:\n",
        "\n",
        "    relevent_docs.add(res['sourceUrl'])\n",
        "for d in relevent_docs:\n",
        "    print(d)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxETH8Sw1HBm",
        "outputId": "d0d8523a-c67c-4675-efe9-8ccb9c2013fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://upload.groundx.ai/file/hsa.sse.a.2_unit%207_lesson%2012.pdf\n",
            "https://upload.groundx.ai/file/7.g.a.1_nwea.pdf\n",
            "https://upload.groundx.ai/file/5.md.c.5.c_nwea.pdf\n",
            "https://upload.groundx.ai/file/2.md.b.5_nwea.pdf\n",
            "https://upload.groundx.ai/file/gr%206.p.5%20quilt%20triangles_final.pdf\n",
            "https://upload.groundx.ai/file/4.md.a.3_parcc.pdf\n",
            "https://upload.groundx.ai/file/5.md.a.1_sbac.pdf\n",
            "https://upload.groundx.ai/file/6.g.a.4_parcc.pdf\n",
            "https://upload.groundx.ai/file/gr%202.p.1%20place%20value%20units_final%20.pdf\n",
            "https://upload.groundx.ai/file/5.nf.b%20multiplication%20and%20division.pdf\n",
            "https://upload.groundx.ai/file/3.md.c.7d_sbac.pdf\n",
            "https://upload.groundx.ai/file/7.g.b.4_nwea.pdf\n",
            "https://upload.groundx.ai/file/5_kde_measurement_and_data_volume_grade_5.pdf\n",
            "https://upload.groundx.ai/file/gr%205.p.2%20forest%20road_final.pdf\n",
            "https://upload.groundx.ai/file/3.md.d.8_nwea.pdf\n",
            "https://upload.groundx.ai/file/6.g.a.4_nwea.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Update\n",
        "while GX is fair at providing relevent examples, I think employing an LLM would be helpful. in outputing more relvent output. So, like RAG, but the user doesn't actually see the LLM response.\n",
        "\n",
        "to make this happen I think I need to first take GXs output and turn it into a slightly different structure which I can feed to an LLM for reranking.\n",
        "\n",
        "Also I noticed that"
      ],
      "metadata": {
        "id": "y9aeUmS342rW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retreival_data = []\n",
        "\n",
        "query=\"give me good math problems for 2nd graders\"\n",
        "\n",
        "response = groundx.search.content(\n",
        "    id=bucket_id,\n",
        "    query=query\n",
        ")\n",
        "\n",
        "for res in response.body['search']['results']:\n",
        "    suggested_text = res['suggestedText']\n",
        "\n",
        "    document_data = groundx.documents.get(document_id = res['documentId']).body\n",
        "    organizational_data = document_data['document']['searchData']\n",
        "    pdf_url = document_data['document']['fileName'].replace(' ','%20')\n",
        "\n",
        "    retreival_data.append({'description_of_document': suggested_text, 'metadata': organizational_data, 'url': pdf_url})\n",
        "\n",
        "# retreival_data"
      ],
      "metadata": {
        "id": "MbxgnhrceEYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "openai.api_key = userdata.get('OpenAIAPIKey')"
      ],
      "metadata": {
        "id": "W3E-mBTI9K3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = openai.chat.completions.create(\n",
        "    model=\"gpt-4o\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"Describe up to five examples which are most relevent to the query. The output should be a list of objects which include the \\\"url\\\", \\\"metadata\\\", and a brief textual description of why it's relevent, called \\\"description\\\". Make the output in a json format. Do not include markdown formatting around the json object. Only include good examples. If less than five examples are good, you may output fewer.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"query: {query}\\n\\n=====example list to be ranked======\\n\\n{retreival_data}\"}\n",
        "    ]\n",
        ")\n",
        "llm_response = response.choices[0].message.content.replace('```json','').replace('```','')"
      ],
      "metadata": {
        "id": "Rodpz0uiI8p5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "print(f'query: {query}\\n\\nresponse:')\n",
        "\n",
        "json.loads(llm_response)"
      ],
      "metadata": {
        "id": "hQdcj1pvnsqg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f03d211b-e820-401c-fd92-3a663823c3b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "query: give me good math problems for 2nd graders\n",
            "\n",
            "response:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'url': 'http://achievethecore.org/content/upload/2.MD.C.8_NWEA.pdf',\n",
              "  'metadata': {'example_index': 60, 'type': 'main example'},\n",
              "  'description': \"This example provides a second-grade math assessment problem involving money and calculations with coins. It is designed to enhance students' understanding of counting and calculating total amounts using different denominations, aligning with the Common Core standard for Measurement and Data. This is highly relevant for second graders as it involves basic arithmetic involving real-life scenarios with coins.\"},\n",
              " {'url': 'http://achievethecore.org/content/upload/Gr%202.P.2%20Two%20snakes_Final%20.pdf',\n",
              "  'metadata': {'example_index': 85,\n",
              "   'link_instance_index': 0,\n",
              "   'link_type_index': 0,\n",
              "   'type': 'linked example'},\n",
              "  'description': 'This document offers a Grade 2 math problem that involves using basic addition and subtraction to solve a word problem comparing the lengths of two snakes. The task aligns with the Common Core standard by requiring computational fluency and an understanding of measurement, making it a suitable choice for second graders.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# futsing with google search to find the exploration\n"
      ],
      "metadata": {
        "id": "yDIEDMN-3lHr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request, json\n",
        "with urllib.request.urlopen(\"https://achievethecore.org/standards-admin/json.php\") as url:\n",
        "    data = url.read().decode()\n",
        "website_data = json.loads(data.replace('window.cc = ',''))"
      ],
      "metadata": {
        "id": "ifPWIVUC4Mub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install googlesearch-python"
      ],
      "metadata": {
        "id": "ANjO8Hck3kEw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbfd07ed-bc50-4173-d13f-bd1b6d9a43e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting googlesearch-python\n",
            "  Downloading googlesearch_python-1.2.5-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: beautifulsoup4>=4.9 in /usr/local/lib/python3.10/dist-packages (from googlesearch-python) (4.12.3)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from googlesearch-python) (2.32.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.9->googlesearch-python) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->googlesearch-python) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->googlesearch-python) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->googlesearch-python) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->googlesearch-python) (2024.8.30)\n",
            "Downloading googlesearch_python-1.2.5-py3-none-any.whl (4.8 kB)\n",
            "Installing collected packages: googlesearch-python\n",
            "Successfully installed googlesearch-python-1.2.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from googlesearch import search\n",
        "from random import randint\n",
        "import urllib.parse\n",
        "\n",
        "def google_search(query):\n",
        "\n",
        "    #constructing an \"I'm feeling lucky\" google search URL\n",
        "    query = urllib.parse.quote('archive the core coherence map ' + query)\n",
        "    return f'http://www.google.com/search?q={query}&btnI'\n",
        "\n",
        "    # Perform the Google search and get the top result\n",
        "    search_results = search(query, num_results=1)\n",
        "\n",
        "    # Return the first result link\n",
        "    for result in search_results:\n",
        "        return result\n",
        "\n",
        "# putting some of the text fields in google search tends to consistently retreive the cohenrence map which is relevent to the main example\n",
        "# query = \"archivethecore coherence map <p>Determine an explicit expression, a recursive process, or steps for calculation from a context.</p>\\r\\n\"\n",
        "# query = \"archivethecore coherence map <p>Understand a <a id=A number expressible in the form a/b where a is a whole number and b is a positive whole number. (The word fraction in these standards always refers to a non-negative number.) See also: rational number.\"\n",
        "# first_link = google_search(query)\n",
        "# first_link"
      ],
      "metadata": {
        "id": "0QpvmTYL3krQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# document_data['document']['searchData']['example_index']"
      ],
      "metadata": {
        "id": "yq_dH86x9TRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example_desc = website_data['standards'][str(document_data['document']['searchData']['example_index'])]['desc']\n",
        "# query = f'archivethecore coherence map {example_desc}'\n",
        "# first_link = google_search(query)\n",
        "# first_link"
      ],
      "metadata": {
        "id": "KVt_ikKB4Jxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Putting it all together\n",
        "constructing the structured output from a query."
      ],
      "metadata": {
        "id": "UtBsVRxB6UqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from pprint import pprint\n",
        "\n",
        "user_query=\"that one example of measuring an eraser\"\n",
        "\n",
        "#documents already uploaded with a modality grounded in text\n",
        "RAG_response = groundx.search.content(\n",
        "    id=bucket_id,\n",
        "    query=user_query\n",
        ")\n",
        "\n",
        "retreival_data = []\n",
        "for res in RAG_response.body['search']['results']:\n",
        "    suggested_text = res['suggestedText']\n",
        "\n",
        "    document_data = groundx.documents.get(document_id = res['documentId']).body\n",
        "    pdf_url = document_data['document']['fileName'].replace(' ','%20')\n",
        "\n",
        "    example_desc = website_data['standards'][str(document_data['document']['searchData']['example_index'])]['desc']\n",
        "    google_query = f'archivethecore coherence map {example_desc}'\n",
        "    first_link = google_search(google_query)\n",
        "\n",
        "    retreival_data.append({'description': suggested_text, 'pdfUrl': pdf_url, 'exploreUrl':first_link})\n",
        "\n",
        "llm_response = openai.chat.completions.create(\n",
        "    model=\"gpt-4o\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"Describe up to five examples which are most relevent to the query. The output should be a list of objects which include the \\\"pdfUrl\\\", \\\"exploreUrl\\\", and a brief textual description of why it's relevent, called \\\"description\\\". Shorten the description to make it 40 words long max for each example. Make the output in a json format. Do not include markdown formatting around the json object. Only include good examples. If less than five examples are good, you may output fewer. The output should be a list at the top level, not a dict.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"query: {user_query}\\n\\n=====example list to be ranked======\\n\\n{retreival_data}\"}\n",
        "    ]\n",
        ")\n",
        "llm_response = llm_response.choices[0].message.content.replace('```json','').replace('```','')\n",
        "structured_data = json.loads(llm_response)\n",
        "formatted_output = {'query': user_query, 'response': structured_data}\n"
      ],
      "metadata": {
        "id": "6WKu7c976a36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(formatted_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c90boJro307g",
        "outputId": "41bab5eb-d280-48f9-a5f5-b8f9aabd2c81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'query': 'that one example of measuring an eraser',\n",
            " 'response': [{'description': 'This resource helps students measure an eraser '\n",
            "                              'in both centimeters and inches, emphasizing '\n",
            "                              'understanding different unit sizes and their '\n",
            "                              'iteration counts.',\n",
            "               'exploreUrl': 'http://www.google.com/search?q=archive%20the%20core%20coherence%20map%20archivethecore%20coherence%20map%20%3Cp%3EMeasure%20the%20length%20of%20an%20object%20twice%2C%20using%20length%20units%20of%20different%20lengths%20for%20the%20two%20measurements%3B%20describe%20how%20the%20two%20measurements%20relate%20to%20the%20size%20of%20the%20unit%20chosen.%3C/p%3E%0D%0A&btnI',\n",
            "               'pdfUrl': 'https://achievethecore.org/content/upload/2.MD.A.2_NWEA.pdf'},\n",
            "              {'description': 'Focuses on measuring and estimating object '\n",
            "                              'lengths, using a ruler. Supports understanding '\n",
            "                              'the concept of standard unit sizes and their '\n",
            "                              'real-world application.',\n",
            "               'exploreUrl': 'http://www.google.com/search?q=archive%20the%20core%20coherence%20map%20archivethecore%20coherence%20map%20%3Cp%3EMeasure%20to%20determine%20how%20much%20longer%20one%20object%20is%20than%20another%2C%20expressing%20the%20length%20difference%20in%20terms%20of%20a%20standard%20length%20unit.%3C/p%3E%0D%0A&btnI',\n",
            "               'pdfUrl': 'https://achievethecore.org/content/upload/2.MD.A.4_NWEA.pdf'},\n",
            "              {'description': 'Educational resource for measuring an eraser '\n",
            "                              'using centimeters and inches, supporting unit '\n",
            "                              'iteration reasoning and understanding.',\n",
            "               'exploreUrl': 'http://www.google.com/search?q=archive%20the%20core%20coherence%20map%20archivethecore%20coherence%20map%20%3Cp%3EExpress%20the%20length%20of%20an%20object%20as%20a%20%3Ca%20id%3D%22The%20numbers%200%2C%201%2C%202%2C%203%2C%20%E2%80%A6.%22%20name%3D%22The%20numbers%200%2C%201%2C%202%2C%203%2C%20%E2%80%A6.%22%3Ewhole%20number%3C/a%3E%20of%20length%20units%2C%20by%20laying%20multiple%20copies%20of%20a%20shorter%20object%20%28the%20length%20unit%29%20end%20to%20end%3B%20understand%20that%20the%20length%20measurement%20of%20an%20object%20is%20the%20number%20of%20same-size%20length%20units%20that%20span%20it%20with%20no%20gaps%20or%20overlaps.%20Limit%20to%20contexts%20where%20the%20object%20being%20measured%20is%20spanned%20by%20a%20whole%20number%20of%20length%20units%20with%20no%20gaps%20or%20overlaps.%3C/p%3E%0D%0A&btnI',\n",
            "               'pdfUrl': 'https://achievethecore.org/content/upload/1.MD.A.2_NWEA.pdf'}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JfZX52GscwA-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}