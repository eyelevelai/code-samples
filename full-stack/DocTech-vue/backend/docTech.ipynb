{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Blocking out the functionality for doc tech\n",
        "This API will:\n",
        "- receive an arbitrary query from the user as a textual transcript from speech to text\n",
        "- will decide which of some number of fixed commands is relevent\n",
        "- will return a structured response based on what is relevent\n",
        "\n",
        "Some things this will do, for instance:\n",
        "- text to page number\n",
        "- text to command for scrolling\n",
        "- text to which PDF to open"
      ],
      "metadata": {
        "id": "hYBz51VhAZkM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bg8QJxW4AY_2",
        "outputId": "bb050b5d-779e-4c81-fc26-3d068eec1cbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langgraph\n",
            "  Downloading langgraph-0.2.35-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.2.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting langchain-core<0.4,>=0.2.39 (from langgraph)\n",
            "  Downloading langchain_core-0.3.10-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting langgraph-checkpoint<3.0.0,>=2.0.0 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.0.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting openai<2.0.0,>=1.40.0 (from langchain-openai)\n",
            "  Downloading openai-1.51.2-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain-openai)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.2.39->langgraph) (6.0.2)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.4,>=0.2.39->langgraph)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.125 (from langchain-core<0.4,>=0.2.39->langgraph)\n",
            "  Downloading langsmith-0.1.134-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.2.39->langgraph) (24.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.2.39->langgraph) (2.9.2)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain-core<0.4,>=0.2.39->langgraph)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.2.39->langgraph) (4.12.2)\n",
            "Collecting msgpack<2.0.0,>=1.1.0 (from langgraph-checkpoint<3.0.0,>=2.0.0->langgraph)\n",
            "  Downloading msgpack-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai<2.0.0,>=1.40.0->langchain-openai)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting jiter<1,>=0.4.0 (from openai<2.0.0,>=1.40.0->langchain-openai)\n",
            "  Downloading jiter-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (4.66.5)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->langchain-openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->langchain-openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.40.0->langchain-openai) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai<2.0.0,>=1.40.0->langchain-openai)\n",
            "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.40.0->langchain-openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.2.39->langgraph)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.2.39->langgraph)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m384.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.2.39->langgraph)\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.2.39->langgraph) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.2.39->langgraph) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.2.3)\n",
            "Downloading langgraph-0.2.35-py3-none-any.whl (108 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.7/108.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-0.2.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.10-py3-none-any.whl (404 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m404.4/404.4 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-2.0.1-py3-none-any.whl (22 kB)\n",
            "Downloading openai-1.51.2-py3-none-any.whl (383 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.7/383.7 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.2/325.2 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading langsmith-0.1.134-py3-none-any.whl (295 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading msgpack-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (378 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m378.0/378.0 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tenacity, orjson, msgpack, jsonpointer, jiter, h11, tiktoken, requests-toolbelt, jsonpatch, httpcore, httpx, openai, langsmith, langchain-core, langgraph-checkpoint, langchain-openai, langgraph\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "  Attempting uninstall: msgpack\n",
            "    Found existing installation: msgpack 1.0.8\n",
            "    Uninstalling msgpack-1.0.8:\n",
            "      Successfully uninstalled msgpack-1.0.8\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.6 httpx-0.27.2 jiter-0.6.1 jsonpatch-1.33 jsonpointer-3.0.0 langchain-core-0.3.10 langchain-openai-0.2.2 langgraph-0.2.35 langgraph-checkpoint-2.0.1 langsmith-0.1.134 msgpack-1.1.0 openai-1.51.2 orjson-3.10.7 requests-toolbelt-1.0.0 tenacity-8.5.0 tiktoken-0.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langgraph langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OpenAIAPIKey')"
      ],
      "metadata": {
        "id": "iDQmGOamDOOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Note:\n",
        "I'm thinking I'll first explore what type of thing I need to do"
      ],
      "metadata": {
        "id": "raphZtqTFS5D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypedDict\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "#action determiner\n",
        "class Action(TypedDict):\n",
        "    scroll_up: bool\n",
        "    scroll_down: bool\n",
        "    next_page: bool\n",
        "    previous_page: bool\n",
        "    snap_page: bool\n",
        "    find_fig: bool\n",
        "    find_pdf: bool\n",
        "    non_determ: bool\n",
        "\n",
        "action_parse_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"\"\"Decide if the user wants one of the following actions performed:\n",
        "            - `scroll_up`: scroll up a bit on the pdf\n",
        "            - `scroll_down`: scroll down a bit on the pdf\n",
        "            - `snap_page`: snap to a specific page of a pdf\n",
        "            - `find_fig`: find a specific figure\n",
        "            - `find_doc`: find a specific doc\n",
        "            - `non_determ`: no valid action is discernable\n",
        "            These are mutually exclusive. One should be true, the rest should be false.\n",
        "            \"\"\",\n",
        "        ),\n",
        "        (\"placeholder\", \"{messages}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "action_parser = action_parse_prompt | ChatOpenAI(\n",
        "    model=\"gpt-4o\", temperature=0\n",
        ").with_structured_output(Action)\n",
        "\n",
        "user_query = 'hey doctor show me a document with that one diagram about a horse'\n",
        "action_parser.invoke({\"messages\": [(\"ai\", \"my name is doc tech, what action would you like me to perform?\"),(\"user\", user_query)]})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KqyXRIgGDP2w",
        "outputId": "9cad5f52-503a-47a7-c657-f8e35f05e745"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'scroll_up': False,\n",
              " 'scroll_down': False,\n",
              " 'next_page': False,\n",
              " 'previous_page': False,\n",
              " 'snap_page': False,\n",
              " 'find_fig': True,\n",
              " 'find_pdf': False,\n",
              " 'non_determ': False}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Note:\n",
        "ok now I need to handle each of these cases."
      ],
      "metadata": {
        "id": "NkYKLsYTJGyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SnapPage(TypedDict):\n",
        "    snap_page: int\n",
        "\n",
        "snap_page_parse_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"\"\"Parse out the specific page of the pdf the user wants to snap to.\n",
        "            \"\"\",\n",
        "        ),\n",
        "        (\"placeholder\", \"{messages}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "snap_page_parser = snap_page_parse_prompt | ChatOpenAI(\n",
        "    model=\"gpt-4o\", temperature=0\n",
        ").with_structured_output(SnapPage)\n",
        "\n",
        "user_query = 'go up two pages'\n",
        "current_page = 6\n",
        "snap_page_parser.invoke({\"messages\": [(\"ai\", f\"my name is doc tech, what page would you like to snap to. You are currently on page {current_page}\"),(\"user\", user_query)]})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrrLUOuRFLBK",
        "outputId": "e80e5baf-291b-461a-9f31-56e191af6c5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'snap_page': 8}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FigDesc(TypedDict):\n",
        "    figure_description: str\n",
        "\n",
        "fig_desc_parse_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"\"\"The user wants to find a figure. Extract a description of the figure the user needs.\n",
        "            \"\"\",\n",
        "        ),\n",
        "        (\"placeholder\", \"{messages}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "fig_desc_parser = fig_desc_parse_prompt | ChatOpenAI(\n",
        "    model=\"gpt-4o\", temperature=0\n",
        ").with_structured_output(FigDesc)\n",
        "\n",
        "user_query = 'hey doc tech, yeah, i uh, i need a diagram of a snow a uh a snow globe with no yea with out a top'\n",
        "fig_desc_parser.invoke({\"messages\": [(\"ai\", f\"my name is doc tech, describe the figure you want me to find.\"),(\"user\", user_query)]})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtDvitxCLmE5",
        "outputId": "25bacb69-b6fd-4fd6-d115-9e12de89cded"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'figure_description': 'a diagram of a snow globe without a top'}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DocDesc(TypedDict):\n",
        "    doc_description: str\n",
        "\n",
        "doc_desc_parse_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"\"\"The user wants to find a document. Extract a description of the document the user needs.\n",
        "            \"\"\",\n",
        "        ),\n",
        "        (\"placeholder\", \"{messages}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "doc_desc_parser = doc_desc_parse_prompt | ChatOpenAI(\n",
        "    model=\"gpt-4o\", temperature=0\n",
        ").with_structured_output(DocDesc)\n",
        "\n",
        "user_query = 'hey doc tech, yeah, i uh, i need a uh status report for twenty 24'\n",
        "doc_desc_parser.invoke({\"messages\": [(\"ai\", f\"my name is doc tech, describe the figure you want me to find.\"),(\"user\", user_query)]})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiUr960jPoC2",
        "outputId": "7a842a78-ac5a-4473-fc27-4504e3847563"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'doc_description': 'status report for 2024'}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Note:\n",
        "I think it would be super useful to be able to understand if the user wants to be on the same pdf or not. So I'll make a parser for that general logic."
      ],
      "metadata": {
        "id": "OHFBjuvRQjPd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SameDoc(TypedDict):\n",
        "    same_doc: bool\n",
        "\n",
        "same_doc_parse_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"\"\"if the user explicitly said they want to do something on this document.\n",
        "            \"\"\",\n",
        "        ),\n",
        "        (\"placeholder\", \"{messages}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "same_doc_parser = same_doc_parse_prompt | ChatOpenAI(\n",
        "    model=\"gpt-4o\", temperature=0\n",
        ").with_structured_output(SameDoc)\n",
        "\n",
        "user_query = 'find that one colorful figure somewhere on this'\n",
        "same_doc_parser.invoke({\"messages\": [(\"ai\", f\"my name is doc tech, describe a figure you want me to find.\"),(\"user\", user_query)]})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RP2c-tJMQMvm",
        "outputId": "40fd161b-964e-44d2-8078-f257e292ac17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'same_doc': True}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Note:\n",
        "ok we're making headway. Now I need a way to search a GX bucket for some specific documents/figures\n",
        "\n",
        "11795"
      ],
      "metadata": {
        "id": "v0SKKrp_SjN9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install groundx-python-sdk --upgrade\n",
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 652
        },
        "id": "6vlOpCOag0_g",
        "outputId": "9731d132-274a-4dd5-ca05-480941604e29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting groundx-python-sdk\n",
            "  Downloading groundx_python_sdk-1.3.27-py3-none-any.whl.metadata (33 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.4 in /usr/local/lib/python3.10/dist-packages (from groundx-python-sdk) (3.10.9)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from groundx-python-sdk) (2024.8.30)\n",
            "Collecting cryptography<43.0.0,>=42.0.5 (from groundx-python-sdk)\n",
            "  Downloading cryptography-42.0.8-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: frozendict<3.0.0,>=2.3.4 in /usr/local/lib/python3.10/dist-packages (from groundx-python-sdk) (2.4.5)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from groundx-python-sdk) (2.8.2)\n",
            "Requirement already satisfied: typing_extensions<5.0.0,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from groundx-python-sdk) (4.12.2)\n",
            "Requirement already satisfied: urllib3<3.0.0,>=1.26.18 in /usr/local/lib/python3.10/dist-packages (from groundx-python-sdk) (2.2.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.4->groundx-python-sdk) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.4->groundx-python-sdk) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.4->groundx-python-sdk) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.4->groundx-python-sdk) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.4->groundx-python-sdk) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.4->groundx-python-sdk) (1.13.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.4->groundx-python-sdk) (4.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography<43.0.0,>=42.0.5->groundx-python-sdk) (1.17.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.8.2->groundx-python-sdk) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography<43.0.0,>=42.0.5->groundx-python-sdk) (2.22)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.4->groundx-python-sdk) (3.10)\n",
            "Downloading groundx_python_sdk-1.3.27-py3-none-any.whl (359 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m359.4/359.4 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cryptography-42.0.8-cp39-abi3-manylinux_2_28_x86_64.whl (3.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: cryptography, groundx-python-sdk\n",
            "  Attempting uninstall: cryptography\n",
            "    Found existing installation: cryptography 43.0.1\n",
            "    Uninstalling cryptography-43.0.1:\n",
            "      Successfully uninstalled cryptography-43.0.1\n",
            "Successfully installed cryptography-42.0.8 groundx-python-sdk-1.3.27\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "cryptography"
                ]
              },
              "id": "b4a69d4c08444d44a1a4af45d1bde704"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "from groundx import Groundx\n",
        "from google.colab import userdata\n",
        "\n",
        "groundx = Groundx(\n",
        "    api_key=userdata.get('GroundXAPIKey_daniel.warfield')\n",
        ")"
      ],
      "metadata": {
        "id": "ezPssTt5g1ZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "from groundx import Groundx\n",
        "\n",
        "bucket_id = 11795\n",
        "\n",
        "def gx_search_figure(query):\n",
        "\n",
        "    response = groundx.search.content(\n",
        "        id=bucket_id,\n",
        "        query=query\n",
        "    )\n",
        "\n",
        "    semantic_object = response.body['search']['results'][0]\n",
        "\n",
        "    return semantic_object['sourceUrl'], semantic_object['boundingBoxes'][0]['pageNumber']\n",
        "\n",
        "gx_search_figure('Theres a colorful block puzzle thing and I cant figure it out')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OND5I8Fqg6x9",
        "outputId": "b0293ac9-c516-4721-8bfd-ea1446e09b22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('https://upload.groundx.ai/file/11795/babys-first-blocks.pdf', 2)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "from groundx import Groundx\n",
        "\n",
        "bucket_id = 11795\n",
        "\n",
        "def gx_search_document(query):\n",
        "\n",
        "    response = groundx.search.content(\n",
        "        id=bucket_id,\n",
        "        query=query\n",
        "    )\n",
        "\n",
        "    semantic_object = response.body['search']['results'][0]\n",
        "\n",
        "    return semantic_object['sourceUrl']\n",
        "\n",
        "gx_search_document('build document of a coffee table')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "3F3_JZXHiRna",
        "outputId": "81357de6-1742-405d-b99d-10b399cc80a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'https://upload.groundx.ai/file/11795/lack-coffee-table-black-brown__aa-472482-3-2.pdf'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tying it all together\n",
        "given a query:\n",
        "- decide on the action to take\n",
        "- do any processing that's required\n",
        "- return a response"
      ],
      "metadata": {
        "id": "_yo501SBnNJ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langgraph langchain-openai\n",
        "!pip install groundx-python-sdk --upgrade\n",
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPCJ44Pxn_pM",
        "outputId": "4035693c-8481-4765-c2ef-8a015f5984c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langgraph\n",
            "  Downloading langgraph-0.2.35-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.2.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting langchain-core<0.4,>=0.2.39 (from langgraph)\n",
            "  Downloading langchain_core-0.3.10-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting langgraph-checkpoint<3.0.0,>=2.0.0 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.0.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting openai<2.0.0,>=1.40.0 (from langchain-openai)\n",
            "  Downloading openai-1.51.2-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain-openai)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.2.39->langgraph) (6.0.2)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.4,>=0.2.39->langgraph)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.125 (from langchain-core<0.4,>=0.2.39->langgraph)\n",
            "  Downloading langsmith-0.1.134-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.2.39->langgraph) (24.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.2.39->langgraph) (2.9.2)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain-core<0.4,>=0.2.39->langgraph)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.2.39->langgraph) (4.12.2)\n",
            "Collecting msgpack<2.0.0,>=1.1.0 (from langgraph-checkpoint<3.0.0,>=2.0.0->langgraph)\n",
            "  Downloading msgpack-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai<2.0.0,>=1.40.0->langchain-openai)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting jiter<1,>=0.4.0 (from openai<2.0.0,>=1.40.0->langchain-openai)\n",
            "  Downloading jiter-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (4.66.5)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->langchain-openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->langchain-openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.40.0->langchain-openai) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai<2.0.0,>=1.40.0->langchain-openai)\n",
            "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.40.0->langchain-openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.2.39->langgraph)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.2.39->langgraph)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.2.39->langgraph)\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.2.39->langgraph) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.2.39->langgraph) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.2.3)\n",
            "Downloading langgraph-0.2.35-py3-none-any.whl (108 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.7/108.7 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-0.2.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.10-py3-none-any.whl (404 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m404.4/404.4 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-2.0.1-py3-none-any.whl (22 kB)\n",
            "Downloading openai-1.51.2-py3-none-any.whl (383 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.7/383.7 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.2/325.2 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading langsmith-0.1.134-py3-none-any.whl (295 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading msgpack-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (378 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m378.0/378.0 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tenacity, orjson, msgpack, jsonpointer, jiter, h11, tiktoken, requests-toolbelt, jsonpatch, httpcore, httpx, openai, langsmith, langchain-core, langgraph-checkpoint, langchain-openai, langgraph\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "  Attempting uninstall: msgpack\n",
            "    Found existing installation: msgpack 1.0.8\n",
            "    Uninstalling msgpack-1.0.8:\n",
            "      Successfully uninstalled msgpack-1.0.8\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.6 httpx-0.27.2 jiter-0.6.1 jsonpatch-1.33 jsonpointer-3.0.0 langchain-core-0.3.10 langchain-openai-0.2.2 langgraph-0.2.35 langgraph-checkpoint-2.0.1 langsmith-0.1.134 msgpack-1.1.0 openai-1.51.2 orjson-3.10.7 requests-toolbelt-1.0.0 tenacity-8.5.0 tiktoken-0.8.0\n",
            "Collecting groundx-python-sdk\n",
            "  Downloading groundx_python_sdk-1.3.27-py3-none-any.whl.metadata (33 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.4 in /usr/local/lib/python3.10/dist-packages (from groundx-python-sdk) (3.10.9)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from groundx-python-sdk) (2024.8.30)\n",
            "Collecting cryptography<43.0.0,>=42.0.5 (from groundx-python-sdk)\n",
            "  Downloading cryptography-42.0.8-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: frozendict<3.0.0,>=2.3.4 in /usr/local/lib/python3.10/dist-packages (from groundx-python-sdk) (2.4.5)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from groundx-python-sdk) (2.8.2)\n",
            "Requirement already satisfied: typing_extensions<5.0.0,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from groundx-python-sdk) (4.12.2)\n",
            "Requirement already satisfied: urllib3<3.0.0,>=1.26.18 in /usr/local/lib/python3.10/dist-packages (from groundx-python-sdk) (2.2.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.4->groundx-python-sdk) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.4->groundx-python-sdk) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.4->groundx-python-sdk) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.4->groundx-python-sdk) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.4->groundx-python-sdk) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.4->groundx-python-sdk) (1.13.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.4->groundx-python-sdk) (4.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography<43.0.0,>=42.0.5->groundx-python-sdk) (1.17.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.8.2->groundx-python-sdk) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography<43.0.0,>=42.0.5->groundx-python-sdk) (2.22)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.4->groundx-python-sdk) (3.10)\n",
            "Downloading groundx_python_sdk-1.3.27-py3-none-any.whl (359 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m359.4/359.4 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cryptography-42.0.8-cp39-abi3-manylinux_2_28_x86_64.whl (3.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: cryptography, groundx-python-sdk\n",
            "  Attempting uninstall: cryptography\n",
            "    Found existing installation: cryptography 43.0.1\n",
            "    Uninstalling cryptography-43.0.1:\n",
            "      Successfully uninstalled cryptography-43.0.1\n",
            "Successfully installed cryptography-42.0.8 groundx-python-sdk-1.3.27\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.51.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.6.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ['GROUNDX_API_KEY'] = userdata.get('GroundXAPIKey_daniel.warfield')\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OpenAIAPIKey')"
      ],
      "metadata": {
        "id": "dKf6QtXNxFbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from groundx import Groundx\n",
        "from google.colab import userdata\n",
        "from typing import TypedDict\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "import os\n",
        "\n",
        "# Setting up API keys\n",
        "groundx = Groundx(\n",
        "    api_key=os.environ['GROUNDX_API_KEY']\n",
        ")\n",
        "\n",
        "#===============================================================================\n",
        "# Action Parsing\n",
        "#===============================================================================\n",
        "\n",
        "#action determiner\n",
        "class Action(TypedDict):\n",
        "    scroll_up: bool\n",
        "    scroll_down: bool\n",
        "    next_page: bool\n",
        "    previous_page: bool\n",
        "    snap_page: bool\n",
        "    find_fig: bool\n",
        "    find_pdf: bool\n",
        "    non_determ: bool\n",
        "\n",
        "action_parse_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"\"\"Decide if the user wants one of the following actions performed:\n",
        "            - `scroll_up`: scroll up a small amount within one page of the pdf\n",
        "            - `scroll_down`: scroll down a small amount within one page of the pdf\n",
        "            - `snap_page`: snap to a specific page of a pdf\n",
        "            - `find_fig`: find a specific figure, table, image, or specific item.\n",
        "            - `find_doc`: find a specific doc\n",
        "            - `non_determ`: no valid action is discernable\n",
        "            These are mutually exclusive. One should be true, the rest should be false.\n",
        "            note: you can use snap_page to go to a page relative to the current page.\n",
        "            note: blanket questions should default to find figure, unless they're obviously about a document\n",
        "            \"\"\",\n",
        "        ),\n",
        "        (\"placeholder\", \"{messages}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "action_parser = action_parse_prompt | ChatOpenAI(\n",
        "    model=\"gpt-4o\", temperature=0\n",
        ").with_structured_output(Action)\n",
        "\n",
        "#===============================================================================\n",
        "# Snap Page Parsing\n",
        "#===============================================================================\n",
        "\n",
        "class SnapPage(TypedDict):\n",
        "    snap_page: int\n",
        "\n",
        "snap_page_parse_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"\"\"Parse out the specific page of the pdf the user wants to snap to.\n",
        "            \"\"\",\n",
        "        ),\n",
        "        (\"placeholder\", \"{messages}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "snap_page_parser = snap_page_parse_prompt | ChatOpenAI(\n",
        "    model=\"gpt-4o\", temperature=0\n",
        ").with_structured_output(SnapPage)\n",
        "\n",
        "#===============================================================================\n",
        "# Figure Description Parsing\n",
        "#===============================================================================\n",
        "\n",
        "class FigDesc(TypedDict):\n",
        "    figure_description: str\n",
        "\n",
        "fig_desc_parse_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"\"\"The user wants to find a figure. Extract a description of the figure the user needs.\n",
        "            \"\"\",\n",
        "        ),\n",
        "        (\"placeholder\", \"{messages}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "fig_desc_parser = fig_desc_parse_prompt | ChatOpenAI(\n",
        "    model=\"gpt-4o\", temperature=0\n",
        ").with_structured_output(FigDesc)\n",
        "\n",
        "#===============================================================================\n",
        "# Document Description Parsing\n",
        "#===============================================================================\n",
        "\n",
        "class DocDesc(TypedDict):\n",
        "    doc_description: str\n",
        "\n",
        "doc_desc_parse_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"\"\"The user wants to find a document. Extract a description of the document the user needs.\n",
        "            \"\"\",\n",
        "        ),\n",
        "        (\"placeholder\", \"{messages}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "doc_desc_parser = doc_desc_parse_prompt | ChatOpenAI(\n",
        "    model=\"gpt-4o\", temperature=0\n",
        ").with_structured_output(DocDesc)\n",
        "\n",
        "user_query = 'hey doc tech, yeah, i uh, i need a uh status report for twenty 24'\n",
        "doc_desc_parser.invoke({\"messages\": [(\"ai\", f\"my name is doc tech, describe the figure you want me to find.\"),(\"user\", user_query)]})\n",
        "\n",
        "#===============================================================================\n",
        "# Search for Figure\n",
        "#===============================================================================\n",
        "\n",
        "bucket_id = 11795\n",
        "\n",
        "def gx_search_figure(query):\n",
        "\n",
        "    response = groundx.search.content(\n",
        "        id=bucket_id,\n",
        "        query=query\n",
        "    )\n",
        "\n",
        "    semantic_object = response.body['search']['results'][0]\n",
        "\n",
        "    return semantic_object['sourceUrl'], semantic_object['boundingBoxes'][0]['pageNumber']\n",
        "\n",
        "#===============================================================================\n",
        "# Search for Documents\n",
        "#===============================================================================\n",
        "\n",
        "def gx_search_document(query):\n",
        "\n",
        "    response = groundx.search.content(\n",
        "        id=bucket_id,\n",
        "        query=query\n",
        "    )\n",
        "\n",
        "    semantic_object = response.body['search']['results'][0]\n",
        "\n",
        "    return semantic_object['sourceUrl']\n",
        "\n",
        "#===============================================================================\n",
        "# Endpoint\n",
        "#===============================================================================\n",
        "\n",
        "def handle_query(query, context):\n",
        "    #getting action that should be performed\n",
        "    response = action_parser.invoke({\"messages\": [(\"ai\", \"my name is doc tech, what action would you like me to perform?\"),(\"user\", query)]})\n",
        "    response['pdf']= None\n",
        "    response['page']=None\n",
        "\n",
        "    #doing follow up as necessary\n",
        "    if response['snap_page']:\n",
        "        response['page']=snap_page_parser.invoke({\"messages\": [(\"ai\", f\"my name is doc tech, what page would you like to snap to. Current state: {context}\"),(\"user\", query)]})\n",
        "    elif response['find_fig']:\n",
        "        response['pdf'], response['page'] = gx_search_figure(query)\n",
        "    elif response['find_pdf']:\n",
        "        response['pdf'] = gx_search_document(query)\n",
        "\n",
        "    return response"
      ],
      "metadata": {
        "id": "aZhOpHdEk9gL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "handle_query('Show me the portfolio overlap of the s&p 500', {'current_page': 6})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZ4cBBqzoJVW",
        "outputId": "32d32c9a-8bb7-424e-802a-95c2b5e15550"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'scroll_up': False,\n",
              " 'scroll_down': False,\n",
              " 'next_page': False,\n",
              " 'previous_page': False,\n",
              " 'snap_page': False,\n",
              " 'find_fig': True,\n",
              " 'find_pdf': False,\n",
              " 'non_determ': False,\n",
              " 'pdf': 'https://upload.groundx.ai/file/11795/dashboard-sp-500-factor.pdf',\n",
              " 'page': 5}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "handle_query('Theres a colorful block puzzle thing and I cant figure it out?', {'current_page': 1})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1LTUjx0rPXg",
        "outputId": "a1fc49c0-0c13-477f-e61f-0be7f825afda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'scroll_up': False,\n",
              " 'scroll_down': False,\n",
              " 'snap_page': False,\n",
              " 'find_fig': True,\n",
              " 'find_pdf': False,\n",
              " 'non_determ': False,\n",
              " 'pdf': 'https://upload.groundx.ai/file/11795/babys-first-blocks.pdf',\n",
              " 'page': 2}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "handle_query('Scroll up', {'current_page': 1})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eh_3MQWsxXYs",
        "outputId": "ccd9e03e-5509-4296-f461-b396f2d51659"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'scroll_up': True,\n",
              " 'scroll_down': False,\n",
              " 'snap_page': False,\n",
              " 'find_fig': False,\n",
              " 'find_pdf': False,\n",
              " 'non_determ': False,\n",
              " 'pdf': None,\n",
              " 'page': None}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "handle_query('Pull up the S and P five hundred document', {'current_page': 1})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XiAFOYCIx0pj",
        "outputId": "ed8e9ed8-43c1-4419-a236-4ab415a27bd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'scroll_up': False,\n",
              " 'scroll_down': False,\n",
              " 'snap_page': False,\n",
              " 'find_fig': False,\n",
              " 'find_pdf': True,\n",
              " 'non_determ': False,\n",
              " 'pdf': 'https://upload.groundx.ai/file/11795/dashboard-sp-500-factor.pdf',\n",
              " 'page': None}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "handle_query('How do I attatch the shelf on this table?', {'current_page': 1})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccUBkeFqx5yh",
        "outputId": "e634342f-7aed-49a8-a52b-4c47b485b832"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'scroll_up': False,\n",
              " 'scroll_down': False,\n",
              " 'snap_page': False,\n",
              " 'find_fig': True,\n",
              " 'find_pdf': False,\n",
              " 'non_determ': False,\n",
              " 'pdf': 'https://upload.groundx.ai/file/11795/lack-coffee-table-black-brown__aa-472482-3-2.pdf',\n",
              " 'page': 7}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HPJgSbpHy2ff"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}